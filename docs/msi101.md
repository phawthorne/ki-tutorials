# Minnesota Supercomputing Institute

Minnesota Supercomputing Institute ([MSI](https://www.msi.umn.edu)) hosts UMN's supercomputers and associated data storage and software. KI researchers have access to MSI computing resources through an umbrella account owned by Jessica. We currently have about 10 TB of primary storage available as well as essentially unlimited computational time.

## What MSI offers


* Computation: MSI has two primary clusters, Mesabi and Mangi. A third, Agate, is coming at some point. There are some differences between the clusters, but Mesabi is a good default choice. Each cluster has a large number of individual nodes (think individual computers) that each have between 24 and 128 cores and up to 2TB of memory. 
* Storage: There are several tiers of storage at MSI ([MSI docs](https://www.msi.umn.edu/content/data-storage)):
    - Primary storage: this is the storage associated with your user account/home directory when you log in. It is high performance but limited in capacity. This is a good place to store working data that needs to be accessed frequently.
    - Tier 2 storage: we have 120 TB of Tier 2 storage, which is accessed through an S3 bucket interface, or application like CyberDuck. This is a good place to store results or data for old projects.
    - Scratch storage: This is temporary storage that can be used by a running job. There are global, local, and RAMdisk scratch locations. Global is very useful, since the files there persist for up to 30 days, but note that as of 2022-01-20 there have been ongoing issues with global scratch performance which can introduce long delays. Local and RAM scratch can still be used, but require you to include a data consolidation step in your job ([MSI docs](https://www.msi.umn.edu/content/scratch-storage))
* Types of access (see docs for [SSH](https://www.msi.umn.edu/content/connecting-hpc-resources) and [interactive](https://www.msi.umn.edu/content/connecting-interactive-hpc-resources) connections)
    * SSH: you can log in to MSI through the command line with `ssh [YOUR X500]@mesabi.msi.umn.edu` (must be on campus or have a VPN active). Note that when you first log in, you are on a "login" node, and should not run computationally demanding scripts. You can use this interface to submit jobs to be run, or to request an interactive session, which will give you a terminal session on a "compute" node (e.g. `srun -N 1 -n 1 -t 4:00:00 -p interactive --tmp 20gb --pty bash`).
    * Various interactive methods, including Desktop GUI and Jupyter notebook options.
    * VSCode: Using the "Remote Development" and associated extensions, you can access, edit, and run code directly on MSI through the VSCode interface. Recommended option!


## Running a job
There is documentation on MSI's site, so this is just a quick introduction/overview. In addition to the code that you need to run, you have to create a Slurm script ([docs](https://www.msi.umn.edu/content/job-submission-and-scheduling-slurm)). Slurm is the software that manages all the jobs running on the cluster, and the Slurm script tells the manager what kind of environment you need (number of cores, total memory, runtime, etc...). This script also needs to call your code along with any additional setup or cleanup commands you need. For example, activating a particular python environment.

Generally relatively small requests will begin running close to immediately, but in any case, once the job is queued you can log out and will be notified by email when your job starts and stops.

## Best Practices
It is likely that you'll need to make some changes to your scripts in order to have them run on MSI:

* File paths and config files: In case you want to be able to run the same code locally and on MSI, you'll run into needing to change filepaths, such as paths for data, between each location. This gets old quickly. An easier solution is to rework your code so that filepaths are specified in a config file that you provide when running the code, and have separate configs for your personal machine and MSI.
* Transferring code: The best way to transfer code to MSI is through github. Create a repository that includes your local code, then clone it to MSI. This way, changes you make in either location can be easily synced to the other.
* Transferring data: Use an application like Cyberduck for smaller transfers, use [Globus](https://www.msi.umn.edu/support/faq/how-do-i-use-globus-transfer-data-second-tier-storage-msi) for larger transfers, or use some other tools like `s3cmd` or `rclone`.
* Software: There are a number of software packages that aren't available by default, but can be activated with a `module load PACKAGE` command. The available applications can be seen [here](https://www.msi.umn.edu/software). 

## Other topics
* Python via Miniconda
* Julia
* Docker -> Singularity
